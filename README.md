# Point-Cloud-Transformer

Trying to improve the performance with ```transformers``` or ```attention models```.
I have found these papers so interesting and useful to read:<br />
"Point Transformer V2: Grouped Vector Attention and Partition-based Pooling" @ https://arxiv.org/abs/2210.05666<br />
"PCGFormer: Lossy Point Cloud Geometry Compression via Local Self-Attention" @ https://ieeexplore.ieee.org/document/10008892<br />

This will be updated soon ...
