# Point-Cloud-Transformer

Trying to improve the performance with ```transformers``` or ```attention models```.
I have found these papers so interesting and useful to read:
"Point Transformer V2: Grouped Vector Attention and Partition-based Pooling" @ https://arxiv.org/abs/2210.05666
"PCGFormer: Lossy Point Cloud Geometry Compression via Local Self-Attention" @ https://ieeexplore.ieee.org/document/10008892

This will be updated soon ...
